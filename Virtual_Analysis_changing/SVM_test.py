# coding=UTF-8
import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import warnings
import pickle
from sklearn import svm #svm导入
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.exceptions import ChangedBehaviorWarning
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import GridSearchCV
import sklearn
import Pcap_class

PREPARE = 2


def distribution_collect(packet):
    meta_data = []
    length = packet.get_packet_head().get_real_len()
    src_p = packet.get_transport_head().get_ports()[0]
    dst_p = packet.get_transport_head().get_ports()[1]
    app_raw = packet.get_application_raw()
    meta_data.append(length)
    meta_data.append(src_p)
    meta_data.append(dst_p)
    for i in range(32):
        if app_raw != None:
            while len(app_raw) < 32:
                  app_raw.append(0)
            meta_data.append(app_raw[i])
        else:
            meta_data.append(0)
    return meta_data


def data_save(data, distribution_dict, packet_No):
    pd_data = pd.DataFrame(data, columns=['length', 'Src_port', 'Dst_port', 'data_01', 'data_02', 'data_03',
                                              'data_04', 'data_05', 'data_06', 'data_07', 'data_08', 'data_09',
                                              'data_10', 'data_11', 'data_12', 'data_13', 'data_14',
                                              'data_15', 'data_16', 'data_17', 'data_18', 'data_19', 'data_20',
                                              'data_21', 'data_22', 'data_23', 'data_24', 'data_25',
                                              'data_26', 'data_27', 'data_28', 'data_29', 'data_30', 'data_31',
                                              'data_32'])
    distribution_dict[packet_No] = pd_data.dropna()


def data_process(x,y):
    X=pd.DataFrame()
    Y=pd.DataFrame()
    for i in x:
        X=X.append(i)
    for i in y:
        Y=Y.append(i)
    X,Y=sklearn.utils.shuffle(X,Y)
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
    return X_train, X_test, y_train, y_test


def fileread(filename,label):
    f = open(filename + '.txt', 'rb')
    X = pickle.load(f)
    Y = pd.DataFrame([label for i in range(X.shape[0])])
    return X, Y


if __name__ == '__main__':
    '''
    mpl.rcParams['font.sans-serif'] = [u'SimHei']
    mpl.rcParams['axes.unicode_minus'] = False
    warnings.filterwarnings('ignore', category=ChangedBehaviorWarning)
    x_w, y_w = fileread("weixin_big_1", 0)
    x_o, y_o = fileread("web_big_1", 1)
    x_q, y_q = fileread("QQ", 2)
    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))
    x_train, x_test, y_train, y_test = data_process([x_w, x_o, x_q], [y_w, y_o, y_q])

    clf = svm.SVC(C=1, kernel='rbf', gamma=20, decision_function_shape='ovr')
    clf.fit(x_train, y_train)
    #print(clf.score(x_train, y_train))
    print('训练集准确率：', accuracy_score(y_train, clf.predict(x_train)))
    sum = 0
    for i in range(50):
        x_train, x_test, y_train, y_test = data_process([x_w, x_o, x_q], [y_w, y_o, y_q])
        #print(clf.score(x_test, y_test))
        #print('测试集准确率：', accuracy_score(y_test, clf.predict(x_test)))
        sum += accuracy_score(y_test, clf.predict(x_test))
    print('测试集平均准确率：', sum/50)
    #print('decision_function:\n', clf.decision_function(x_train))
    #print('\npredict:\n', clf.predict(x_train))
    # # 以写二进制的方式打开文件
    file = open("test2.model", "wb")
    # # 把模型写入到文件中
    pickle.dump(clf, file)
    # # 关闭文件
    file.close()
    '''

#'''
    # 以读二进制的方式打开文件
    file = open("test2.model", "rb")
    # 把模型从文件中读取出来
    anova_svm = pickle.load(file)
    # 关闭文件
    file.close()
    filename = "D:/Projects/Virtual_Analysis_changing/weixin_big_2"
    my_pcap_file = Pcap_class.build_pcap(filename + ".pcap", 150)
    packets = my_pcap_file.get_packets()
    packet_list = []
    packet_num = 0
    for packet in packets:
        data = distribution_collect(packet)
        #print(data)
        packet_list.append(data)
        packet_num += 1
        if packet_num > 150:
            break
    test_data = pd.DataFrame(packet_list, columns=['length', 'Src_port', 'Dst_port', 'data_01', 'data_02', 'data_03',
                                              'data_04', 'data_05', 'data_06', 'data_07', 'data_08', 'data_09',
                                              'data_10', 'data_11', 'data_12', 'data_13', 'data_14',
                                              'data_15', 'data_16', 'data_17', 'data_18', 'data_19', 'data_20',
                                              'data_21', 'data_22', 'data_23', 'data_24', 'data_25',
                                              'data_26', 'data_27', 'data_28', 'data_29', 'data_30', 'data_31',
                                              'data_32'])
    prediction = anova_svm.predict(test_data)
    print('decision_function:\n', anova_svm.decision_function(test_data))
    print('\npredict:\n',anova_svm.predict(test_data))
#'''
'''
    filename = "QQ"
    if PREPARE == 0:
        x_r,y_r = fileread("QQ",1)
        x_w,y_w = fileread("test1",0)
        scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))
        X_train, X_test, y_train, y_test = data_process([x_r, x_w], [y_r, y_w])
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.fit_transform(X_test)
        X_train = X_train.reshape((X_train.shape[0], 1, 14))
        X_test = X_test.reshape((X_test.shape[0], 1, 14))
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.LSTM(32, input_shape=(1, 14)))
        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy',
                      optimizer='rmsprop',
                      metrics=['accuracy'])
        model.fit(X_train, y_train, batch_size=16, epochs=50)
        score, acc = model.evaluate(X_test, y_test, batch_size=32)
        print(score)
        print(acc)
        model.save("test.model")
    elif PREPARE == 2:
        filename = "QQ2"
        my_pcap_file = Pcap_class.build_pcap(filename + ".pcap", 30)
        packets = my_pcap_file.get_packets()
        packet_list = []
        packet_num = 0
        for packet in packets:
            data = distribution_collect(packet)
            packet_list.append(data)
            packet_num += 1
            if packet_num > 40:
                break
        model = tf.keras.models.load_model('test.model')
        test_data = pandas.DataFrame(packet_list, columns=['length', 'Src_port', 'Dst_port', 'data_01', 'data_02', 'data_03'
            , 'data_04', 'data_05', 'data_06', 'data_07', 'data_08', 'data_09'
            , 'data_10', 'data_11'])
        test_data = test_data.values
        test_data = test_data.reshape((test_data.shape[0], 1, 14))
        prediction = model.predict(test_data)
        #prediction = model.predict(test_data)
        print(prediction)
'''

